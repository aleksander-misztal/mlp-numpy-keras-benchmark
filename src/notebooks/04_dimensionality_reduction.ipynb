{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f2776f",
   "metadata": {},
   "source": [
    "\n",
    "## PCA Evaluation on Most Linearly Correlated Feature Pair\n",
    "\n",
    "### Objective\n",
    "To evaluate whether PCA on the most linearly correlated pair of input features improves classification performance compared to the baseline model.\n",
    "\n",
    "### Method\n",
    "- Identified the most linearly correlated feature pair: `Oldpeak` and `ExerciseAngina` (corr ≈ 0.41).\n",
    "- Applied PCA with `n_components=1` to reduce these two features into a single principal component.\n",
    "- Trained a Random Forest classifier using this single component.\n",
    "- Compared results to the baseline Random Forest model trained on the full set of encoded features.\n",
    "\n",
    "### Result\n",
    "- The PCA component retained the shared linear variance between the two features.\n",
    "- However, performance was lower than the full model, which uses the richness of all available features.\n",
    "- The PCA transformation provided compact input but at the cost of losing independent information from other features.\n",
    "\n",
    "### Conclusion\n",
    "PCA on the most linearly correlated pair (`Oldpeak`, `ExerciseAngina`) provides a compact representation of shared variance but does not outperform a full-featured Random Forest. It confirms that PCA is useful for dimensionality reduction when compressing redundant features, but not necessarily beneficial for predictive accuracy when working with few or weakly dependent inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8775a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed3590b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/heart_feature_engineering.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eaa3bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak',\n",
       "       'HeartDisease', 'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina',\n",
       "       'ST_Slope', 'RestingBP_missing', 'Cholesterol_missing',\n",
       "       'Oldpeak_missing', 'CholesterolPerAge', 'HRRatio',\n",
       "       'Sex_FastingBS_Freq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b0069",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "205c6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='HeartDisease')\n",
    "y = df['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Train/Test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Baseline model using all features ---\n",
    "rf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "y_pred_base = rf_baseline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "071132b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (All Features) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86        82\n",
      "           1       0.88      0.91      0.89       102\n",
      "\n",
      "    accuracy                           0.88       184\n",
      "   macro avg       0.88      0.88      0.88       184\n",
      "weighted avg       0.88      0.88      0.88       184\n",
      "\n",
      "Confusion Matrix:\n",
      "[[69 13]\n",
      " [ 9 93]]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Baseline (All Features) ===\")\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cff62",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_compress = ['Oldpeak', 'ExerciseAngina']\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a4aa37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train[features_to_compress])\n",
    "X_test_scaled = scaler.transform(X_test[features_to_compress])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "41d83521",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "90e706c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_final = X_train.drop(columns=features_to_compress).copy()\n",
    "X_test_pca_final = X_test.drop(columns=features_to_compress).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "75f59e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_final['Oldpeak_Exercise_PCA'] = X_train_pca\n",
    "X_test_pca_final['Oldpeak_Exercise_PCA'] = X_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2ac0ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pca = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_pca.fit(X_train_pca_final, y_train)\n",
    "y_pred_pca = rf_pca.predict(X_test_pca_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e64efaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model After PCA (2→1 replacement) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88        82\n",
      "           1       0.89      0.91      0.90       102\n",
      "\n",
      "    accuracy                           0.89       184\n",
      "   macro avg       0.89      0.89      0.89       184\n",
      "weighted avg       0.89      0.89      0.89       184\n",
      "\n",
      "Confusion Matrix:\n",
      "[[71 11]\n",
      " [ 9 93]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Model After PCA (2→1 replacement) ===\")\n",
    "print(classification_report(y_test, y_pred_pca))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9314a68",
   "metadata": {},
   "source": [
    "## Model Evaluation Summary: Feature Engineering and PCA Impact\n",
    "\n",
    "### Objective\n",
    "\n",
    "To evaluate the impact of engineered features and PCA-based dimensionality reduction on classification performance using Random Forest.  \n",
    "This experiment serves as a **baseline diagnostic**, with **no hyperparameter tuning** applied.\n",
    "\n",
    "---\n",
    "\n",
    "### Experimental Scenarios\n",
    "\n",
    "We evaluated the model in four distinct setups:\n",
    "\n",
    "1. **Baseline** — original features only  \n",
    "2. **Baseline + PCA (2→1)** — `Oldpeak` and `ExerciseAngina` replaced with a PCA component  \n",
    "3. **Engineered Features** — original + new features (`CholesterolPerAge`, `HRRatio`, `Sex_FastingBS_Freq`)  \n",
    "4. **Engineered + PCA (2→1)** — PCA reduction applied as in (2)\n",
    "\n",
    "---\n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### Scenario 1: Original Features Only\n",
    "- Accuracy: 0.90\n",
    "- Precision: 0 = 0.90, 1 = 0.90\n",
    "- Recall: 0 = 0.88, 1 = 0.92\n",
    "- F1-score (macro): 0.90\n",
    "- Confusion Matrix:\n",
    "\n",
    "  [[72 10]\n",
    "\n",
    "   [ 8 94]]\n",
    "\n",
    "#### Scenario 2: Original + PCA (2→1)\n",
    "- Accuracy: 0.90\n",
    "- Precision: 0 = 0.91, 1 = 0.90\n",
    "- Recall: 0 = 0.87, 1 = 0.93\n",
    "- F1-score (macro): 0.90\n",
    "- Confusion Matrix:\n",
    "\n",
    "  [[71 11]\n",
    "\n",
    "   [ 7 95]]\n",
    "\n",
    "#### Scenario 3: Engineered Features (no PCA)\n",
    "- Accuracy: 0.88\n",
    "- Precision: 0 = 0.88, 1 = 0.88\n",
    "- Recall: 0 = 0.84, 1 = 0.91\n",
    "- F1-score (macro): 0.88\n",
    "- Confusion Matrix:\n",
    "\n",
    "  [[69 13]\n",
    "\n",
    "   [ 9 93]]\n",
    "\n",
    "#### Scenario 4: Engineered + PCA (2→1)\n",
    "- Accuracy: 0.91\n",
    "- Precision: 0 = 0.91, 1 = 0.90\n",
    "- Recall: 0 = 0.88, 1 = 0.93\n",
    "- F1-score (macro): 0.91\n",
    "- Confusion Matrix:\n",
    "\n",
    "  [[72 10]\n",
    "\n",
    "   [ 7 95]]\n",
    "\n",
    "---\n",
    "\n",
    "### Final Notes\n",
    "\n",
    "- Adding new features alone introduced redundancy or noise that slightly degraded performance.\n",
    "- Replacing `Oldpeak` and `ExerciseAngina` with a PCA component helped remove this redundancy, enabling the new features to contribute effectively.\n",
    "- The highest performance was achieved in the combined scenario: **engineered features + PCA(2→1)**.\n",
    "- This experiment highlights how **selective dimensionality reduction** can unlock the value of complex feature spaces.\n",
    "- No hyperparameter optimization was performed — results reflect baseline model behavior.\n",
    "- While a production-grade pipeline (e.g., `scikit-learn.Pipeline`) would typically be used for clean feature processing and modeling, this workflow was kept manual to facilitate transparent experimentation and interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/heart_feature_engineering.csv')\n",
    "\n",
    "# 1. Select columns\n",
    "cols_to_pca = ['Oldpeak', 'ExerciseAngina']\n",
    "\n",
    "# 2. Standardize data (PCA requires standardized data)\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df[cols_to_pca])\n",
    "\n",
    "# 3. PCA from 2 to 1 component\n",
    "pca = PCA(n_components=1)\n",
    "df['Oldpeak_Exercise_PCA'] = pca.fit_transform(scaled)\n",
    "\n",
    "# 4. Drop original columns (optional)\n",
    "df.drop(columns=cols_to_pca, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "613705f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/heart_processed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71027b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
